---
title: "flaxRettingMetaomics_litReview"
author: "Jakob Vucelic-Frick"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(bibliometrix)
library(tidyverse)
library(vcd)
library(treemap)
library(stringr)
library(igraph)
library(ggraph)
library(rvest)
```


# README: an Introduction

The [*Biblioverse*](https://www.bibliometrix.org/home/index.php/layout/biblioverse) has a collection of packages designed to perform systematic analyses of research publications of interest. The main packages include *bibliometrix*, *dimensionsR*,*pubmedR*, and *openalexR*. They essensially serve as API services to acsess publication metadata from their respective databases. Refer to this [link](https://www.bibliometrix.org/vignettes/Introduction_to_bibliometrix.html) for an introduction to *bibliometrix*.
This file will attempt to use the *Biblioverse* as a means to find relevant research pertaining to the microbiota associated with dew-retting flax, hemp, and other related plants. 
Also, if a web-UI is more convenient, you can run *biblioshiny()* to access the shiny app. 


# A Practical Application of the *Biblioverse*

### [dimensionsR](http://github.com/massimoaria/dimensionsR)

You will need to register with an institutional email to use their API. Refer to this [link](https://digital-science.ccgranttracker.com/login.aspx). After registration you'll then need to submit an application describing your project.

```{r install_dimensionsR}
#install.packages("dimensionsR")
library(dimensionsR)
```

```{r}
# with login info
token <- dsAuth(username = "your_username", password = "your_password")

## or temp API key 
#token <- dsAuth(key = "your_apikey")

# write the querey

query <- dsQueryBuild(item = "publications", 
                  words = "bibliometric*", 
                  type = "article", 
                  categories = "management", 
                  start_year = 1980, end_year = 2020,
                  output_fields = c("basics", "extras", "authors", "concepts"))
                  
#you will need to apply for use to get the credentials
dsAuth(username = "<your username>", password = "<your password>")
```


### [pubmedR](https://github.com/massimoaria/pubmedR)

```{r install_pubmedR}
#install.packages("pubmedR")
library(pubmedR)
```

The *pubmedR* workflow consists of 4 steps:
1. Construct the query
2. Check the validity of the query
3. Download the documents' metadata
4. Convert the metadata to a usable format

Thankfully, the NCBI API system is free and allows users to make 3 queries a second or 10/sec if the user registers for an API key. To register for a key, use this [link](https://www.ncbi.nlm.nih.gov/account/) and access your account settings. You can also refer to this [PDF](https://cran.r-project.org/web/packages/pubmedR/pubmedR.pdf) for the package's documentation. 
I found using this [page](https://pubmed.ncbi.nlm.nih.gov/advanced/) was helpful while constructing queries. It follows a pretty straightforward format, but it is helpful to see the options and their respective codes. This [link](https://pubmed.ncbi.nlm.nih.gov/help/) is also helpful for a full set of documentation (hint: ctrl+f and search "search field tags").

```{r pubmedR_demo, eval=F}
### 1) Generate credentials ###

## if you have an API key, store it
#api_key <- "your API key"

# otherwise, set the key value to null
api_key = NULL


### 2) construct and test query ###

## Construct the query

query <- "bibliometric*[Title/Abstract] AND english[LA] AND Journal Article[PT] AND 2000:2020[DP]"

# test the query
res <- pmQueryTotalCount(query = query, api_key = api_key)

#	The total number of records returned by the query
res$total_count

#The query translation by the NCBI Automatic Terms Translation system
res$query_translation

# The web history object. The NCBI provides search history features, which is useful for dealing with large lists of IDs or repeated searches.
res$web_history

### 3) download the collection of metadata ###

# it is good practice to specify the total number of documents from the querey
metaData <- pmApiRequest(query = query, limit = res$total_count, api_key = api_key)

# The xml-structured list containing the bibliographic metadata collection downloaded from the PubMed database.
mdData <- metaData$data

# the original query
mdQuery <- metaData$query

# the query, translated by the NCBI Automatic Terms Translation system and submitted to the PubMed database.
mdQueryTrans <- metaData$query_translation

#the total number of records downloaded and stored in "data".
mdRecordsDownloaded <- metaData$records_downloaded

# the number of records matching the query (stored in the "query_translation" object").
mdRecordsMatched <- metaData$total_counts


### 4) convert to readable/useable format ###

#default format is bibliometrix (for bibliometrix compatibility), if set to "raw" then it converts to a dataframe with no additional data handling 
biblioForm <- pmApi2df(metaData, format="bibliometrix")
dfMetadata <- pmApi2df(metaData, format="raw")


M <- convert2df(metaData, dbsource = "pubmed", format = "api")

results <- biblioAnalysis(M)

# summary of the bibliometric analysis
S <- summary(results)

#plot of the results
plot(x=results)
topAU <- authorProdOverTime(M, k = 10, graph = TRUE)
```

```{r pubmedR_flaxApplication}
### 1) Generate credentials ###

api_key = NULL

### 2) construct and test query ###

## Construct the query
generalQuery <- "(flax[tiab] OR hemp[tiab] or rett*[tiab]) AND microb*[tiab] NOT (Rett syndrome[tiab])"

specificQuery <- "((flax[tiab] OR hemp[tiab] OR kenaf[tiab] OR rett*[tiab]) AND (community[tiab] OR metaomic*[tiab] OR R16[tiab] OR R18[tiab] OR ITS[tiab]) AND microb*[tiab]) NOT (Rett syndrome[tiab])"

# test the queries
resGen <- pmQueryTotalCount(query = generalQuery, api_key = api_key)
resGen$total_count

resSpec <- pmQueryTotalCount(query = specificQuery, api_key = api_key)
resSpec$total_count


### 3) download the collection of metadata ###

# it is good practice to specify the total number of documents from the querey
genData <- pmApiRequest(query = generalQuery, limit = resGen$total_count, api_key = api_key)

specData <- pmApiRequest(query = specificQuery, limit = resSpec$total_count, api_key = api_key)


### 4) convert to readable/useable format ###

dfGen <- convert2df(genData, dbsource = "pubmed", format = "api")
dfSpec <- convert2df(specData, dbsource = "pubmed", format = "api")

genRes <- biblioAnalysis(dfGen)
specRes <- biblioAnalysis(dfSpec)

# summary of the bibliometric analysis
S <- summary(genRes)


```

# START AI CONTENT: CHAT GPT WAS USED TO GENERATE THE CODE BELOW
```{r analysis}
# Split authors into rows
author_counts <- dfSpec %>%
  separate_rows(AU, sep = ";") %>%     # Split the authors by ";"
  mutate(AU = trimws(AU)) %>%     # Trim any whitespace
  count(AU, name = "publications") %>%# Count the number of occurrences
  filter(publications >1)

# Plot
ggplot(author_counts, aes(x = reorder(AU, -publications), y = publications)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Number of Publications per Author",
       x = "Author",
       y = "Number of Publications") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


# keyword analysis
df <- dfSpec[,c("ID","DE")]

# Step 1: Merge both keyword columns
df_keywords <- df %>%
  unite("all_keywords", ID, DE, sep = ";") %>%           # Combine the two columns
  mutate(all_keywords = str_replace_all(all_keywords, "^;+|;+$", ""),       # Remove leading/trailing semicolons
         all_keywords = str_split(all_keywords, ";"))                  # Split into list of keywords

# Step 2: Create co-occurrence pairs (if statement catches error caused by combn when there are less than 2 keywords)
keyword_pairs <- df_keywords %>%
  pull(all_keywords) %>%
  lapply(function(kw) {
    kw <- unique(trimws(kw))  # Clean and deduplicate
    if (length(kw) >= 2) {
      combn(kw, 2, simplify = FALSE)
    } else {
      NULL  # Skip rows with <2 keywords
    }
  }) %>%
  unlist(recursive = FALSE) %>%
  do.call(rbind, .) %>%
  as.data.frame(stringsAsFactors = FALSE)

colnames(keyword_pairs) <- c("from", "to")

# Step 3: Count pair co-occurrences
edges <- keyword_pairs %>%
  group_by(from, to) %>%
  summarise(weight = n(), .groups = "drop")

# Step 4: Create graph and plot network
graph <- graph_from_data_frame(edges, directed = FALSE)

ggraph(graph, layout = "fr") +
  geom_edge_link(aes(width = weight), edge_alpha = 0.5) +
  geom_node_point(size = 5, color = "steelblue") +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void() +
  labs(title = "Keyword Co-occurrence Network")

#note: i think the network layout is randomised so consider setting seed
#also, there is probally a better way to show the network, maybe only doing the top keywords?

#make table for treemap
all_keywords_flat <- df_keywords %>%
  unnest(all_keywords) %>%
  mutate(keyword = trimws(all_keywords)) %>%
  count(keyword, sort = TRUE)
```
# END AI CONTENT

## Code Sourced from the [R Graph Gallery](https://r-graph-gallery.com/)
```{r}
dfCountries <- as.data.frame(table(dfSpec$SO_CO))
colnames(dfCountries) <- c("Country", "Number of Publications")
# Create data

# treemap
treemap(dfCountries,
            index="Country",
            vSize="Number of Publications",
            type="index"
            )

#consider removing keywords with 1 occurrence
treemap(all_keywords_flat,
            index="keyword",
            vSize="n",
            type="index"
            )


# Most basic bubble plot
dfSpec$PY <- as.Date(paste0(dfSpec$PY, "-01-01"))
tbYears <- as.data.frame(table(dfSpec$PY))
p <- ggplot(tbYears, aes(x=Var1, y=Freq)) +
  geom_line(group=1) + 
  xlab("Year")+
  ylab("Number of Publications")+
  ggtitle("Number of Publications Over Time")
p
```

# Code to extract the abstract and doi to more readable format

```{r writeAbstractsToHTML}
# Template HTML with "Insert List of Info" to be replaced with the paper information
htmlTemplate <- '<body><div id="MathJax_Message" style="display: none;"></div><div id="MathJax_Message" style="display: none;"></div><div class="container-fluid main-container"><div id="header"><h1 class="title toc-ignore">flaxRettingMetaomics_litReview</h1><h4 class="author">Jakob Vucelic-Frick</h4><h4 class="date">2025-05-10</h4></div><div class="section level1" id="list-of-abstracts">Insert List of Info</div></div><script>// add bootstrap table styles to pandoc tablesfunction bootstrapStylePandocTables() {  $(\'tr.odd\').parent(\'tbody\').parent(\'table\').addClass(\'table table-condensed\');}$(document).ready(function () {  bootstrapStylePandocTables();});</script><!-- tabsets --><script>$(document).ready(function () {  window.buildTabsets("TOC");});$(document).ready(function () {  $(\'.tabset-dropdown > .nav-tabs > li\').click(function () {    $(this).parent().toggleClass(\'nav-tabs-open\');  });});</script><!-- code folding --><!-- dynamically load mathjax for compatibility with self-contained --><script>  (function () {    var script = document.createElement("script");    script.type = "text/javascript";    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";    document.getElementsByTagName("head")[0].appendChild(script);  })();</script></body>'

# for each record, write the title, DOI, date, and abstract to html
htmlText <- ""
for (rec in 1:nrow(dfSpec)){
  title <- dfSpec[rec,"TI"]
  ab <- str_to_sentence(dfSpec[rec,"AB"])
  py <- substr(dfSpec[rec,"PY"], 1,4)
  doi <- dfSpec[rec,"DI"]
  htmlText <- paste0(htmlText,"<h3>",title,": DOI-",doi," Year: ",py,"</h3>","<p>",ab,"</p>")
}

# replace the text in the template
htmlToWrite <- gsub("Insert List of Info", htmlText, htmlTemplate)

#write to html file in wd
htmlOutputFile<-file("listOfAbstracts.html")
writeLines(htmlToWrite, htmlOutputFile)
close(htmlOutputFile)
```


### [openalexR](https://github.com/massimoaria/openalexR)

```{r}
#install.packages("openalexR")
library(openalexR)
```

```{r}
works_from_dois <- oa_fetch(
  entity = "works",
  doi = c("10.1016/j.joi.2017.08.007", "https://doi.org/10.1007/s11192-013-1221-3"),
  verbose = TRUE
)

works_from_dois |>
  show_works() |>
  knitr::kable()

works_from_pmids <- oa_fetch(
  entity = "works",
  pmid = c("14907713", 32572199),
  verbose = TRUE
)
#> Requesting url: https://api.openalex.org/works?filter=pmid%3A14907713%7C32572199
#> Getting 1 page of results with a total of 2 records...
works_from_pmids |>
  show_works() |>
  knitr::kable()


works_from_orcids <- oa_fetch(
  entity = "works",
  author.orcid = c("0000-0001-6187-6610", "0000-0002-8517-9411"),
  verbose = TRUE
)

works_from_orcids |>
  show_works() |>
  knitr::kable()
```






# Citations (thank God for all the smart people)

Aria, M. & Cuccurullo, C. (2017) bibliometrix: An R-tool for comprehensive science mapping analysis, 
                                  Journal of Informetrics, 11(4), pp 959-975, Elsevier.
